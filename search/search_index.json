{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pneuma: LLM-Based Data Discovery System for Tabular Data","text":"<p><code>Pneuma</code> is an LLM-powered data discovery system for tabular data. Given a natural language query, <code>Pneuma</code> searches an indexed collection and retrieves the most relevant tables for the question. It performs this search by leveraging both content (columns and rows) and context (metadata) to match tables with questions.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you would like to try <code>Pneuma</code> without installation, you can use our Colab notebook. For local installation, you may use an OpenAI API token or a local GPU with at least 20 GB of VRAM (to load and prompt both the LLM and embedding model).</p>"},{"location":"api/index_generator/","title":"IndexGenerator","text":""},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator","title":"IndexGenerator","text":"<p>Generates indexes for content summaries and context (metadata) associated with tables.</p> <p>This class provides a method to create hybrid---vector &amp; full-text---indexes that helps efficiently organize information to be queried later.</p>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator--attributes","title":"Attributes","text":"<ul> <li>embedding_model (<code>OpenAI | SentenceTransformer</code>): The model used for text embeddings.</li> <li>db_path (<code>str</code>): Path to the database file for retrieving content summaries &amp; context.</li> <li>index_path (<code>str</code>): Path to the directory where indexes are stored.</li> <li>stemmer (<code>Stemmer</code>): A stemming tool used for text normalization.</li> <li>vector_index_path (<code>str</code>): Path for vector-based indexing.</li> <li>fulltext_index_path (<code>str</code>): Path for full-text search indexing.</li> <li>EMBEDDING_MAX_TOKENS (<code>int</code>): The maximum number of tokens the embedding model supports (hard-coded to 512 for local models and 8191 for OpenAI models).</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__init__","title":"__init__","text":"<pre><code>__init__(\n    embed_model: OpenAI | SentenceTransformer, db_path: str, index_path: str\n)\n</code></pre>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.generate_index","title":"generate_index","text":"<pre><code>generate_index(\n    index_name: str, table_ids: list[str] | tuple[str] = None\n) -&gt; str\n</code></pre> <p>Generates a hybrid index with name <code>index_name</code> for a given <code>table_ids</code>.</p>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.generate_index--args","title":"Args","text":"<ul> <li>index_name (<code>str</code>): The name of the index to be generated.</li> <li>table_ids (<code>list[str] | tuple[str]</code>): The IDs of tables to be indexed (to be exact, their content summaries &amp; context/metadata).</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.generate_index--returns","title":"Returns","text":"<ul> <li><code>str</code>: A JSON string representing the result of the process (<code>Response</code>).</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__generate_vector_index","title":"__generate_vector_index","text":"<pre><code>__generate_vector_index(index_name: str, chroma_client: ClientAPI) -&gt; Response\n</code></pre> <p>Generates a vector index with name <code>index_name</code> using <code>ChromaDB-Deterministic</code>.</p>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__generate_vector_index--args","title":"Args","text":"<ul> <li>index_name (<code>str</code>): The name of the index to be generated.</li> <li>chroma_client (<code>ClientAPI</code>): A Client API for <code>ChromaDB-Deterministic</code>.</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__generate_vector_index--returns","title":"Returns","text":"<ul> <li><code>Response</code>: A <code>Response</code> object of the process.</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__insert_documents_to_vector_index","title":"__insert_documents_to_vector_index","text":"<pre><code>__insert_documents_to_vector_index(\n    index_id: int,\n    table_ids: list[str] | tuple[str],\n    chroma_collection: Collection,\n) -&gt; Response\n</code></pre> <p>Inserts documents (related to the tables associated with <code>table_ids</code>) into a vector index.</p>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__insert_documents_to_vector_index--args","title":"Args","text":"<ul> <li>index_id (<code>int</code>): The database ID of the vector index.</li> <li>table_ids (<code>list[str] | tuple[str]</code>): The IDs of tables to be indexed (to be exact, their content summaries &amp; context/metadata).</li> <li>chroma_collection (<code>Collection</code>): The vector index.</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__insert_documents_to_vector_index--returns","title":"Returns","text":"<ul> <li><code>Response</code>: A <code>Response</code> object of the process.</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__generate_fulltext_index","title":"__generate_fulltext_index","text":"<pre><code>__generate_fulltext_index(index_name: str)\n</code></pre> <p>Generates a full-text index with name <code>index_name</code> using <code>BM25s</code>.</p>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__generate_fulltext_index--args","title":"Args","text":"<ul> <li>index_name (<code>str</code>): The name of the index to be generated.</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__generate_fulltext_index--returns","title":"Returns","text":"<ul> <li><code>Response</code>: A <code>Response</code> object of the process.</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__insert_documents_to_fulltext_index","title":"__insert_documents_to_fulltext_index","text":"<pre><code>__insert_documents_to_fulltext_index(\n    index_id: int, table_ids: list | tuple, retriever: BM25\n)\n</code></pre> <p>Inserts documents (related to the tables associated with <code>table_ids</code>) into a full-text index.</p>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__insert_documents_to_fulltext_index--args","title":"Args","text":"<ul> <li>index_id (<code>int</code>): The database ID of the full-text index.</li> <li>table_ids (<code>list[str] | tuple[str]</code>): The IDs of tables to be indexed (to be exact, their content summaries &amp; context/metadata).</li> <li>retriever (<code>BM25</code>): The full-text index.</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__insert_documents_to_fulltext_index--returns","title":"Returns","text":"<ul> <li><code>Response</code>: A <code>Response</code> object of the process.</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__get_table_contexts","title":"__get_table_contexts","text":"<pre><code>__get_table_contexts(table_id: str) -&gt; list[tuple[str, str]]\n</code></pre> <p>Retrieves all contexts (metadata) associated with <code>table_id</code> from the database.</p>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__get_table_contexts--args","title":"Args","text":"<ul> <li>table_id (<code>str</code>): The ID of the table in the database.</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__get_table_contexts--returns","title":"Returns","text":"<ul> <li><code>list[tuple[str, str]]</code>: The contexts and their associated IDs.</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__merge_contexts","title":"__merge_contexts","text":"<pre><code>__merge_contexts(contexts: list[tuple[str, str]]) -&gt; list[str]\n</code></pre>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__get_content_summaries","title":"__get_content_summaries","text":"<pre><code>__get_content_summaries(\n    table_id: str, summary_type: SummaryType\n) -&gt; list[tuple[str, str]]\n</code></pre> <p>Retrieves all content summaries associated with <code>table_id</code> from the database.</p>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__get_content_summaries--args","title":"Args","text":"<ul> <li>table_id (<code>str</code>): The ID of the table in the database.</li> <li>summary_type (<code>SummaryType</code>): The type of summaries to be retrieved (either column narration or row sample).</li> </ul>"},{"location":"api/index_generator/#pneuma.index_generator.IndexGenerator.__get_content_summaries--returns","title":"Returns","text":"<ul> <li><code>list[tuple[str, str]]</code>: The content summaries and their associated IDs.</li> </ul>"},{"location":"api/query_processor/","title":"QueryProcessor","text":""},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor","title":"QueryProcessor","text":"<p>Processes queries against generated hybrid indexes.</p> <p>This class provides a method to retrieve tables from hybrid indexes, helping people find the relevant tables for their tasks.</p>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor--attributes","title":"Attributes","text":"<ul> <li>pipe (<code>OpenAI | TextGenerationPipeline</code>): The LLM pipeline for inference.</li> <li>embedding_model (<code>OpenAI | SentenceTransformer</code>): The model used for text embeddings.</li> <li>stemmer (<code>Stemmer</code>): A stemming tool used for text normalization.</li> <li>index_path (<code>str</code>): Path to the directory where indexes are stored.</li> <li>vector_index_path (<code>str</code>): Path for vector-based indexing.</li> <li>fulltext_index_path (<code>str</code>): Path for full-text search indexing.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__init__","title":"__init__","text":"<pre><code>__init__(\n    llm: OpenAI | TextGenerationPipeline,\n    embed_model: OpenAI | SentenceTransformer,\n    index_path: str,\n)\n</code></pre>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.query","title":"query","text":"<pre><code>query(\n    index_name: str,\n    queries: str | list[str],\n    k: int = 1,\n    n: int = 5,\n    alpha: float = 0.5,\n) -&gt; str\n</code></pre> <p>Retrieves tables for the given <code>queries</code> against the index <code>index_name</code>.</p>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.query--args","title":"Args","text":"<ul> <li>index_name (<code>str</code>): The name of the index to be retrieved against.</li> <li>queries (<code>str | list[str]</code>): The query of list of queries to be executed.</li> <li>k (<code>int</code>): The number of documents associated with the tables to be retrieved.</li> <li>n (<code>int</code>): The multiplicative factor of <code>k</code> to pool more relevant documents for the hybrid retrieval process.</li> <li>alpha (<code>float</code>): The weighting factor of the vector and full-text retrievers within a hybrid index. Lower <code>alpha</code> gives more weight to the vector retriever.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.query--returns","title":"Returns","text":"<ul> <li><code>str</code>: A JSON string representing the result of the process (<code>Response</code>).</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__get_retrievers","title":"__get_retrievers","text":"<pre><code>__get_retrievers(index_name: str) -&gt; tuple[Collection, bm25s.BM25]\n</code></pre> <p>Get both vector and full-text retrievers of the index <code>index_name</code>.</p>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__get_retrievers--args","title":"Args","text":"<ul> <li>index_name (<code>str</code>): The name of the hybrid index.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__get_retrievers--returns","title":"Returns","text":"<ul> <li><code>tuple[Collection, bm25s.BM25]</code>: The vector and full-text retrievers.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__hybrid_retriever","title":"__hybrid_retriever","text":"<pre><code>__hybrid_retriever(\n    bm25_retriever: BM25,\n    vec_retriever: Collection,\n    bm25_res: tuple[ndarray, ndarray],\n    vec_res: QueryResult,\n    k: int,\n    query: str,\n    alpha: float,\n    query_tokens: Tokenized,\n    query_embedding: list[float],\n) -&gt; list[tuple[str, float, str]]\n</code></pre> <p>Generates a hybrid index with name <code>index_name</code> for a given <code>table_ids</code>.</p>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__hybrid_retriever--args","title":"Args","text":"<ul> <li>bm25_retriever (<code>BM25</code>): The full-text retriever within the hybrid index.</li> <li>vec_retriever (<code>Collection</code>): The vector retriever within the hybrid index.</li> <li>bm25_res (<code>tuple[ndarray, ndarray]</code>): Retrieval results from the full-text retriever.</li> <li>vec_res (<code>QueryResult</code>): Retrieval results from the vector retriever.</li> <li>k (<code>int</code>): The number of documents retrieved from both retrievers.</li> <li>query (<code>str</code>): The query.</li> <li>alpha (<code>float</code>): The weighting factor of the vector and full-text retrievers within a hybrid index. Lower <code>alpha</code> gives more weight to the vector retriever.</li> <li>query_tokens (<code>Tokenized</code>): The tokenized query.</li> <li>query_embeddings (<code>list[float]</code>): The embedding of the query</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__hybrid_retriever--returns","title":"Returns","text":"<ul> <li><code>list[tuple[str, float, str]]</code>: The result of the hybrid search.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__process_nodes_bm25","title":"__process_nodes_bm25","text":"<pre><code>__process_nodes_bm25(\n    items: tuple[ndarray, ndarray],\n    missing_ids: list[str],\n    dictionary_id_bm25: dict[str, int],\n    bm25_retriever: BM25,\n    query_tokens: Tokenized,\n)\n</code></pre> <p>Processes the retrieval results of the full-text retriever for the purpose of hybrid search (augment the results with missing IDs of documents retrieved from the vector index).</p>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__process_nodes_bm25--args","title":"Args","text":"<ul> <li>items (<code>tuple[ndarray, ndarray]</code>): Retrieval results from the full-text retriever.</li> <li>missing_ids (<code>list[str]</code>): The IDs available in the retrieval results of the vector retriever but not full-text retriever.</li> <li>dictionary_id_bm25 (<code>dict[str, int]</code>): The table-document associations within the full-text retriever.</li> <li>bm25_retriever (<code>BM25</code>): The full-text retriever.</li> <li>query_tokens (<code>Tokenized</code>): The tokenized query.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__process_nodes_bm25--returns","title":"Returns","text":"<ul> <li><code>dict[str, tuple[float, str]]</code>: The processed results representing the score and document of each document ID.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__process_nodes_vec","title":"__process_nodes_vec","text":"<pre><code>__process_nodes_vec(\n    items: QueryResult,\n    missing_ids: list[str],\n    collection: Collection,\n    query_embedding: list[float],\n)\n</code></pre> <p>Processes the retrieval results of the vector retriever for the purpose of hybrid search (augment the results with missing IDs of documents retrieved from the full-text index).</p>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__process_nodes_vec--args","title":"Args","text":"<ul> <li>items (<code>QueryResult</code>): Retrieval results from the vector retriever.</li> <li>missing_ids (<code>list[str]</code>): The IDs available in the retrieval results of the full-text retriever but not vector retriever.</li> <li>collection (<code>dict[str, int]</code>): The vector retriever</li> <li>query_embedding (<code>list[float]</code>): The embedding of the query.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__process_nodes_vec--returns","title":"Returns","text":"<ul> <li><code>dict[str, tuple[float, str]]</code>: The processed results representing the score and document of each document ID.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__rerank","title":"__rerank","text":"<pre><code>__rerank(\n    nodes: list[tuple[str, float, str]], query: str\n) -&gt; list[tuple[str, float, str]]\n</code></pre> <p>Perform re-ranking of documents against the query. Basically, the <code>LLM Judge</code> classifies whether a document is relevant or not against the query.</p>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__rerank--args","title":"Args","text":"<ul> <li>nodes (<code>list[tuple[str, float, str]]</code>): The list of tuples, each of which consists of ID, relevance score, and document, resulted from the hybrid search mechanism.</li> <li>query (<code>str</code>): The query.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__rerank--returns","title":"Returns","text":"<ul> <li><code>dict[str, tuple[float, str]]</code>: The re-ranked results.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__get_relevance_prompt","title":"__get_relevance_prompt","text":"<pre><code>__get_relevance_prompt(desc: str, desc_type: str, query: str)\n</code></pre> <p>Returns relevance prompts for re-ranking purposes. The prompt format is slightly different between content summaries and context (metadata).</p>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__get_relevance_prompt--args","title":"Args","text":"<ul> <li>desc (<code>str</code>): The description of a table, which is either a content summary or context (metadata).</li> <li>desc_type (<code>str</code>): The description type: content or context.</li> <li>query (<code>str</code>): The query to be compared against.</li> </ul>"},{"location":"api/query_processor/#pneuma.query_processor.QueryProcessor.__get_relevance_prompt--returns","title":"Returns","text":"<ul> <li><code>str</code>: A relevance prompt.</li> </ul>"},{"location":"api/reference/","title":"Overview","text":""},{"location":"api/reference/#pneuma.Pneuma","title":"Pneuma","text":"<p>The entry point of <code>Pneuma</code>, combining all modules for ther purpose of LLM-based table discovery.</p> <p>This class provides end-to-end methods from indexing tables (and their metadata, if any) to retrieving tables given users' queries.</p>"},{"location":"api/reference/#pneuma.Pneuma--attributes","title":"Attributes","text":"<ul> <li>out_path (<code>str</code>): The output folder of Pneuma.</li> <li>db_path (<code>str</code>): The database path within the output folder of Pneuma.</li> <li>index_location (<code>str</code>): The index path within the output folder of Pneuma.</li> <li>hf_token (<code>str</code>): A HuggingFace User Access Tokens.</li> <li>openai_api_key (<code>str</code>): An OpenAI API key.</li> <li>use_local_model (<code>bool</code>): The option to use local or third-party models (for now, OpenAI models only as both LLM and embedding model).</li> <li>llm_path (<code>str</code>): The path or name of a local LLM from HuggingFace.</li> <li>embed_path (<code>str</code>): The path or name of a local embedding model from HuggingFace.</li> <li>max_llm_batch_size (<code>int</code>): Maximum batch size for the dynamic batch size selector to explore.</li> <li>registrar (<code>Registrar</code>): The dataset regisration module.</li> <li>summarizer (<code>Summarizer</code>): The dataset summarizer module.</li> <li>index_generator (<code>IndexGenerator</code>): The index generator module.</li> <li>query_processor (<code>QueryProcessor</code>): The query processor module.</li> <li>llm (<code>OpenAI | TextGenerationPipeline</code>): The actual LLM (lazily initialized).</li> <li>embed_model (<code>OpenAI | SentenceTransformer</code>): The actual embedding model (lazily initialized).</li> </ul>"},{"location":"api/reference/#pneuma.Pneuma.__init__","title":"__init__","text":"<pre><code>__init__(\n    out_path: Optional[str] = None,\n    hf_token: Optional[str] = None,\n    openai_api_key: Optional[str] = None,\n    use_local_model: bool = True,\n    llm_path: str = \"Qwen/Qwen2.5-7B-Instruct\",\n    embed_path: str = \"BAAI/bge-base-en-v1.5\",\n    max_llm_batch_size: int = 50,\n)\n</code></pre>"},{"location":"api/reference/#pneuma.Pneuma.setup","title":"setup","text":"<pre><code>setup() -&gt; str\n</code></pre> <p>Setup Pneuma through its <code>Registrar</code> module.</p>"},{"location":"api/reference/#pneuma.Pneuma.add_tables","title":"add_tables","text":"<pre><code>add_tables(\n    path: str,\n    creator: str,\n    source: str = \"file\",\n    s3_region: str = None,\n    s3_access_key: str = None,\n    s3_secret_access_key: str = None,\n    accept_duplicates: bool = False,\n) -&gt; str\n</code></pre> <p>Registers tables into the database by utilizing the <code>Registrar</code> module.</p>"},{"location":"api/reference/#pneuma.Pneuma.add_tables--returns","title":"Returns","text":"<ul> <li><code>str</code>: A JSON string representing the result of the process (<code>Response</code>).</li> </ul>"},{"location":"api/reference/#pneuma.Pneuma.add_metadata","title":"add_metadata","text":"<pre><code>add_metadata(metadata_path: str, table_id: str = '') -&gt; str\n</code></pre> <p>Registers metadata into the database by utilizing the <code>Registrar</code> module.</p>"},{"location":"api/reference/#pneuma.Pneuma.add_metadata--returns","title":"Returns","text":"<ul> <li><code>str</code>: A JSON string representing the result of the process (<code>Response</code>).</li> </ul>"},{"location":"api/reference/#pneuma.Pneuma.summarize","title":"summarize","text":"<pre><code>summarize(table_id: str = None) -&gt; str\n</code></pre> <p>Summarizes the contents of all unsummarized tables or a specific table if <code>table_id</code> is provided using the <code>Summarizer</code> module.</p>"},{"location":"api/reference/#pneuma.Pneuma.summarize--args","title":"Args","text":"<ul> <li>table_id (<code>str</code>): The specific table ID to be summarized.</li> </ul>"},{"location":"api/reference/#pneuma.Pneuma.summarize--returns","title":"Returns","text":"<ul> <li><code>str</code>: A JSON string representing the result of the process (<code>Response</code>).</li> </ul>"},{"location":"api/reference/#pneuma.Pneuma.generate_index","title":"generate_index","text":"<pre><code>generate_index(\n    index_name: str, table_ids: list[str] | tuple[str] = None\n) -&gt; str\n</code></pre> <p>Generates a hybrid index with name <code>index_name</code> for a given <code>table_ids</code> by utilizing the <code>IndexGenerator</code> module.</p>"},{"location":"api/reference/#pneuma.Pneuma.generate_index--args","title":"Args","text":"<ul> <li>index_name (<code>str</code>): The name of the index to be generated.</li> <li>table_ids (<code>list[str] | tuple[str]</code>): The IDs of tables to be indexed (to be exact, their content summaries &amp; context/metadata).</li> </ul>"},{"location":"api/reference/#pneuma.Pneuma.generate_index--returns","title":"Returns","text":"<ul> <li><code>str</code>: A JSON string representing the result of the process (<code>Response</code>).</li> </ul>"},{"location":"api/reference/#pneuma.Pneuma.query_index","title":"query_index","text":"<pre><code>query_index(\n    index_name: str,\n    queries: list[str] | str,\n    k: int = 1,\n    n: int = 5,\n    alpha: int = 0.5,\n) -&gt; str\n</code></pre> <p>Retrieves tables for the given <code>queries</code> against the index <code>index_name</code> by utilizing the <code>QueryProcessor</code> module.</p>"},{"location":"api/reference/#pneuma.Pneuma.query_index--args","title":"Args","text":"<ul> <li>index_name (<code>str</code>): The name of the index to be retrieved against.</li> <li>queries (<code>str | list[str]</code>): The query of list of queries to be executed.</li> <li>k (<code>int</code>): The number of documents associated with the tables to be retrieved.</li> <li>n (<code>int</code>): The multiplicative factor of <code>k</code> to pool more relevant documents for the hybrid retrieval process.</li> <li>alpha (<code>float</code>): The weighting factor of the vector and full-text retrievers within a hybrid index. Lower <code>alpha</code> gives more weight to the vector retriever.</li> </ul>"},{"location":"api/reference/#pneuma.Pneuma.query_index--returns","title":"Returns","text":"<ul> <li><code>str</code>: A JSON string representing the result of the process (<code>Response</code>).</li> </ul>"},{"location":"api/reference/#pneuma.Pneuma.__hf_login","title":"__hf_login","text":"<pre><code>__hf_login()\n</code></pre> <p>Logs into Hugging Face if a token is provided.</p>"},{"location":"api/reference/#pneuma.Pneuma.__init_registrar","title":"__init_registrar","text":"<pre><code>__init_registrar()\n</code></pre> <p>Initializes the Registrar module.</p>"},{"location":"api/reference/#pneuma.Pneuma.__init_summarizer","title":"__init_summarizer","text":"<pre><code>__init_summarizer()\n</code></pre> <p>Initializes the Summarizer module.</p>"},{"location":"api/reference/#pneuma.Pneuma.__init_index_generator","title":"__init_index_generator","text":"<pre><code>__init_index_generator()\n</code></pre> <p>Initializes the IndexGenerator module.</p>"},{"location":"api/reference/#pneuma.Pneuma.__init_query_processor","title":"__init_query_processor","text":"<pre><code>__init_query_processor()\n</code></pre> <p>Initializes the QueryProcessor module.</p>"},{"location":"api/reference/#pneuma.Pneuma.__init_llm","title":"__init_llm","text":"<pre><code>__init_llm()\n</code></pre> <p>Initializes the LLM.</p>"},{"location":"api/reference/#pneuma.Pneuma.__init_embed_model","title":"__init_embed_model","text":"<pre><code>__init_embed_model()\n</code></pre> <p>Initializes the embedding model.</p>"},{"location":"api/registrar/","title":"Registrar","text":""},{"location":"api/registrar/#pneuma.registrar.Registrar","title":"Registrar","text":"<p>Registers dataset and its context (metadata) to the database.</p> <p>This class provides methods to setup the registration system and to register tables &amp; context (metadata).</p>"},{"location":"api/registrar/#pneuma.registrar.Registrar--attributes","title":"Attributes","text":"<ul> <li>db_path (<code>str</code>): Path to the database file for retrieving content summaries &amp; context.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__init__","title":"__init__","text":"<pre><code>__init__(db_path: str)\n</code></pre>"},{"location":"api/registrar/#pneuma.registrar.Registrar.setup","title":"setup","text":"<pre><code>setup() -&gt; str\n</code></pre> <p>Setups the database system for registration purposes.</p>"},{"location":"api/registrar/#pneuma.registrar.Registrar.setup--returns","title":"Returns","text":"<ul> <li><code>str</code>: A JSON string representing the result of the process (<code>Response</code>).</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.add_tables","title":"add_tables","text":"<pre><code>add_tables(\n    path: str,\n    creator: str,\n    source: str = \"file\",\n    s3_region: str = None,\n    s3_access_key: str = None,\n    s3_secret_access_key: str = None,\n    accept_duplicates: bool = False,\n) -&gt; str\n</code></pre> <p>Adds tables into the database.</p>"},{"location":"api/registrar/#pneuma.registrar.Registrar.add_tables--args","title":"Args","text":"<ul> <li>path (<code>str</code>): The path to a specific table file/folder (<code>CSV</code> or <code>parquet</code>).</li> <li>creator (<code>str</code>): The creator of the file.</li> <li>source (<code>str</code>): The dataset source (either <code>file</code> or <code>s3</code>).</li> <li>s3_region (<code>int</code>): Amazon S3 region.</li> <li>s3_access_key (<code>int</code>): Amazon S3 access key.</li> <li>s3_secret_access_key (<code>int</code>): Amazon S3 secret access key.</li> <li>accept_duplicates (<code>bool</code>): Option to accept duplicate tables or not.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.add_tables--returns","title":"Returns","text":"<ul> <li><code>str</code>: A JSON string representing the result of the process (<code>Response</code>).</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.add_metadata","title":"add_metadata","text":"<pre><code>add_metadata(metadata_path: str, table_id: str = '') -&gt; str\n</code></pre> <p>Adds metadata into the database.</p>"},{"location":"api/registrar/#pneuma.registrar.Registrar.add_metadata--args","title":"Args","text":"<ul> <li>metadata_path (<code>str</code>): The path to a specific metadata file/folder (<code>TXT</code> or <code>CSV</code>).</li> <li>table_id (<code>str</code>): A specific table id associated with the metadata.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.add_metadata--returns","title":"Returns","text":"<ul> <li><code>str</code>: A JSON string representing the result of the process (<code>Response</code>).</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_table_file","title":"__read_table_file","text":"<pre><code>__read_table_file(\n    path: str, creator: str, accept_duplicates: bool = False\n) -&gt; Response\n</code></pre> <p>Reads a table file (CSV or Parquet), registers it in the database, and updates if an existing table has the same ID.</p>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_table_file--args","title":"Args","text":"<ul> <li>path (<code>str</code>): The path to a specific table file (<code>CSV</code> or <code>parquet</code>).</li> <li>creator (<code>str</code>): The creator of the file.</li> <li>accept_duplicates (<code>bool</code>): Option to allow duplicate tables or not.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_table_file--returns","title":"Returns","text":"<ul> <li><code>Response</code>: A <code>Response</code> object of the process.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_table_folder","title":"__read_table_folder","text":"<pre><code>__read_table_folder(\n    folder_path: str, creator: str, accept_duplicates: bool = False\n) -&gt; Response\n</code></pre> <p>Reads a folder and registers all of its tables to the database.</p>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_table_folder--args","title":"Args","text":"<ul> <li>folder_path (<code>str</code>): The path to a folder containing tables.</li> <li>creator (<code>str</code>): The creator of the file.</li> <li>accept_duplicates (<code>bool</code>): Option to allow duplicate tables or not.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_table_folder--returns","title":"Returns","text":"<ul> <li><code>Response</code>: A <code>Response</code> object of the process.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_metadata_file","title":"__read_metadata_file","text":"<pre><code>__read_metadata_file(metadata_path: str, table_id: str) -&gt; Response\n</code></pre> <p>Reads a metadata file (CSV or TXT) and registers it in the database.</p>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_metadata_file--args","title":"Args","text":"<ul> <li>metadata_path (<code>str</code>): The path to a specific metadata file (<code>CSV</code> or <code>TXT</code>).</li> <li>table_id (<code>str</code>): The associated table of the metadata.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_metadata_file--returns","title":"Returns","text":"<ul> <li><code>Response</code>: A <code>Response</code> object of the process.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_metadata_folder","title":"__read_metadata_folder","text":"<pre><code>__read_metadata_folder(metadata_path: str, table_id: str) -&gt; Response\n</code></pre> <p>Reads a folder and registers all of its metadata to the database.</p>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_metadata_folder--args","title":"Args","text":"<ul> <li>metadata_path (<code>str</code>): The path to a folder containing metadata files.</li> <li>table_id (<code>str</code>): The associated table of the metadata files.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__read_metadata_folder--returns","title":"Returns","text":"<ul> <li><code>Response</code>: A <code>Response</code> object of the process.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__insert_metadata","title":"__insert_metadata","text":"<pre><code>__insert_metadata(metadata_content: str, table_id: str) -&gt; Response\n</code></pre> <p>Inserts metadata associated with table <code>table_id</code> into the database.</p>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__insert_metadata--args","title":"Args","text":"<ul> <li>metadata_content (<code>str</code>): The content of the metadata.</li> <li>table_id (<code>str</code>): The associated table of the metadata.</li> </ul>"},{"location":"api/registrar/#pneuma.registrar.Registrar.__insert_metadata--returns","title":"Returns","text":"<ul> <li><code>Response</code>: A <code>Response</code> object of the process.</li> </ul>"},{"location":"api/summarizer/","title":"Summarizer","text":""},{"location":"api/summarizer/#pneuma.summarizer.Summarizer","title":"Summarizer","text":"<p>Summarizes indexed tables in the database.</p> <p>This class provides a method to summarize indexed tables in the database to represent them for retrieval purposes.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer--attributes","title":"Attributes","text":"<ul> <li>pipe (<code>OpenAI | TextGenerationPipeline</code>): The LLM pipeline for inference.</li> <li>embedding_model (<code>OpenAI | SentenceTransformer</code>): The model used for text embeddings.</li> <li>db_path (<code>str</code>): Path to the database file for retrieving content summaries &amp; context.</li> <li>MAX_LLM_BATCH_SIZE (<code>int</code>): The upper bound of batch size value to explore dynamically for LLM inference.</li> <li>EMBEDDING_MAX_TOKENS (<code>int</code>): The maximum number of tokens the embedding model supports (hard-coded to 512 for local models and 8191 for OpenAI models).</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__init__","title":"__init__","text":"<pre><code>__init__(\n    llm: OpenAI | TextGenerationPipeline,\n    embed_model: OpenAI | SentenceTransformer,\n    db_path: str,\n    max_llm_batch_size: int = 50,\n)\n</code></pre>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.summarize","title":"summarize","text":"<pre><code>summarize(table_id: str = None) -&gt; str\n</code></pre> <p>Summarizes the contents of all unsummarized tables or a specific table if <code>table_id</code> is provided.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.summarize--args","title":"Args","text":"<ul> <li>table_id (<code>str</code>): The specific table ID to be summarized.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.summarize--returns","title":"Returns","text":"<ul> <li><code>str</code>: A JSON string representing the result of the process (<code>Response</code>).</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__summarize_table_by_id","title":"__summarize_table_by_id","text":"<pre><code>__summarize_table_by_id(table_id: str) -&gt; list[str]\n</code></pre> <p>Summarizes the contents of a single table: <code>table_id</code>.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__summarize_table_by_id--args","title":"Args","text":"<ul> <li>table_id (<code>str</code>): The specific table ID to be summarized.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__summarize_table_by_id--returns","title":"Returns","text":"<ul> <li><code>list[str]</code>: The database IDs of the resulting summaries for table <code>table_id</code>.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__batch_summarize_tables","title":"__batch_summarize_tables","text":"<pre><code>__batch_summarize_tables(table_ids: list[str]) -&gt; list[str]\n</code></pre> <p>Summarizes the contents of tables <code>table_ids</code>.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__batch_summarize_tables--args","title":"Args","text":"<ul> <li>table_ids (<code>list[str]</code>): The specific table IDs to be summarized.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__batch_summarize_tables--returns","title":"Returns","text":"<ul> <li><code>list[str]</code>: The database IDs of the resulting summaries for the tables.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__generate_column_narrations","title":"__generate_column_narrations","text":"<pre><code>__generate_column_narrations(df: DataFrame) -&gt; list[str]\n</code></pre> <p>Generate column narrations for a single dataframe (for quick local testing). This method may be removed in the future.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__batch_generate_column_narrations","title":"__batch_generate_column_narrations","text":"<pre><code>__batch_generate_column_narrations(\n    table_ids: list[str],\n) -&gt; dict[str, list[str]]\n</code></pre> <p>Generates column narrations for the tables <code>table_ids</code>.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__batch_generate_column_narrations--args","title":"Args","text":"<ul> <li>table_ids (<code>list[str]</code>): The specific table IDs to be narrated.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__batch_generate_column_narrations--returns","title":"Returns","text":"<ul> <li><code>dict[str, list[str]]</code>: The column narrations of the tables.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__get_col_narration_prompt","title":"__get_col_narration_prompt","text":"<pre><code>__get_col_narration_prompt(columns: str, column: str) -&gt; str\n</code></pre> <p>Returns the prompt to narrate a column of a table given other columns in the table.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__get_col_narration_prompt--args","title":"Args","text":"<ul> <li>columns (<code>str</code>): A concatenation of <code>columns</code>.</li> <li>column (<code>str</code>): A specific column (part of <code>columns</code>) to be narrated.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__get_col_narration_prompt--returns","title":"Returns","text":"<ul> <li><code>str</code>: The prompt to narrate <code>column</code>.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__get_optimal_batch_size","title":"__get_optimal_batch_size","text":"<pre><code>__get_optimal_batch_size(conversations: list[dict[str, str]]) -&gt; int\n</code></pre> <p>Explores the optimal batch size value (bounded between 1 and <code>MAX_LLM_BATCH_SIZE</code>) for <code>conversations</code> to be set for the LLM pipeline using binary search.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__get_optimal_batch_size--args","title":"Args","text":"<ul> <li>conversations (<code>list[dict[str, str]]</code>): The list of prompts to narrate columns of tables.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__get_optimal_batch_size--returns","title":"Returns","text":"<ul> <li><code>int</code>: The optimal batch size.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__is_fit_in_memory","title":"__is_fit_in_memory","text":"<pre><code>__is_fit_in_memory(\n    conversations: list[dict[str, str]], batch_size: int\n) -&gt; bool\n</code></pre> <p>Checks if <code>conversations</code> with the given <code>batch_size</code> fits in memory when running inference using the LLM pipeline.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__is_fit_in_memory--args","title":"Args","text":"<ul> <li>conversations (<code>list[dict[str, str]]</code>): The list of prompts to narrate columns of tables.</li> <li>batch_size (<code>int</code>): The specific batch size value to test.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__is_fit_in_memory--returns","title":"Returns","text":"<ul> <li><code>bool</code>: The <code>conversations</code> fit or not in memory.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__get_special_indices","title":"__get_special_indices","text":"<pre><code>__get_special_indices(prompts: list[str], batch_size: int) -&gt; list[int]\n</code></pre> <p>Sorts <code>prompts</code> in a specific manner to try to balance the memory load for each batch of LLM inferences.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__get_special_indices--args","title":"Args","text":"<ul> <li>prompts (<code>list[str]</code>): The list of prompts to narrate columns of tables.</li> <li>batch_size (<code>int</code>): The optimal batch size value to be used.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__get_special_indices--returns","title":"Returns","text":"<ul> <li><code>list[int]</code>: The \"special indices\" for the <code>prompts</code> given the <code>batch size</code>.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__block_column_narrations","title":"__block_column_narrations","text":"<pre><code>__block_column_narrations(column_narrations: list[str]) -&gt; list[str]\n</code></pre> <p>Convert column narrations into blocks to try to group multiple narrations as much as possible, reducing the amount of embeddings that need to be produced.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__block_column_narrations--args","title":"Args","text":"<ul> <li>column_narrations (<code>list[str]</code>): The list of column narrations for a set of tables.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__block_column_narrations--returns","title":"Returns","text":"<ul> <li><code>list[str]</code>: The blocked version of <code>column_narrations</code>.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__generate_row_samples","title":"__generate_row_samples","text":"<pre><code>__generate_row_samples(df: DataFrame) -&gt; list[str]\n</code></pre> <p>Generates row samples for the table <code>df</code>. The process is deterministic because we set the sampling seed to be the value 0.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__generate_row_samples--args","title":"Args","text":"<ul> <li>df (<code>pd.DataFrame</code>): The specific table to sample rows from.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__generate_row_samples--returns","title":"Returns","text":"<ul> <li><code>list[str]</code>: The sampled rows</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__block_row_samples","title":"__block_row_samples","text":"<pre><code>__block_row_samples(row_samples: list[str]) -&gt; list[str]\n</code></pre> <p>Convert row samples into blocks to try to group multiple samples as much as possible, reducing the amount of embeddings that need to be produced.</p>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__block_row_samples--args","title":"Args","text":"<ul> <li>row_samples (<code>list[str]</code>): The list of row samples for a set of tables.</li> </ul>"},{"location":"api/summarizer/#pneuma.summarizer.Summarizer.__block_row_samples--returns","title":"Returns","text":"<ul> <li><code>list[str]</code>: The blocked version of <code>row_samples</code>.</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>To install the latest stable release from PyPI:</p> <pre><code>$ pip install pneuma\n</code></pre> <p>To install the most recent version from the repository:</p> <pre><code>$ git clone https://github.com/TheDataStation/Pneuma.git\n$ cd Pneuma\n$ pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#installation-note","title":"Installation Note","text":"<p>To ensure smooth installation and usage, we strongly recommend installing <code>Miniconda</code> (follow this). Then, create a new environment and install the CUDA Toolkit:</p> <pre><code>$ conda create --name pneuma python=3.12.2 -y\n$ conda activate pneuma\n$ conda install -c nvidia cuda-toolkit -y\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start Tutorial","text":"<p>The simplest way to explore <code>Pneuma</code> is by running the quickstart Jupyter notebook. This notebook walks you through <code>Pneuma</code>'s full workflow, from data registration to querying. For those eager to dive in, here\u2019s a snippet showcasing its functionality:</p> <pre><code>from src.pneuma import Pneuma\n\n# Initialize Pneuma\nout_path = \"out_demo/storage\"\npneuma = Pneuma(\n    out_path=out_path,\n    llm_path=\"Qwen/Qwen2.5-7B-Instruct\",\n    embed_path=\"BAAI/bge-base-en-v1.5\",\n)\npneuma.setup()\n\n# Register dataset &amp; summarize it\ndata_path = \"data_src/sample_data/csv\"\npneuma.add_tables(path=data_path, creator=\"demo_user\")\npneuma.summarize()\n\n# Add context (metadata) if available\nmetadata_path = \"data_src/sample_data/metadata.csv\"\npneuma.add_metadata(metadata_path=metadata_path)\n\n# Generate index\npneuma.generate_index(index_name=\"demo_index\")\n\n# Query the index\nresponse = pneuma.query_index(\n    index_name=\"demo_index\",\n    query=\"Which dataset contains climate issues?\",\n    k=1,\n    n=5,\n    alpha=0.5,\n)\nresponse = json.loads(response)\nquery = response[\"data\"][\"query\"]\nretrieved_tables = response[\"data\"][\"response\"]\n</code></pre>"}]}