{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneuma: Quick Start (Colab)\n",
    "\n",
    "In this notebook, we show how to use each of Pneuma's features, from registering a dataset to querying the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Before we proceed, let's install Pneuma & download the test dataset.\n",
    "\n",
    "---\n",
    "**Colab will ask you to restart your session after the installation in order for certain dependencies to setup properly. Click the button `Restart Session` and continue from the next cell in the notebook**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pneuma\n",
    "!pip install pneuma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data\n",
    "!gdown \"1NN_TxpgBlCjC_ZEBgOnBPMY0CxEX-_EL\" -O \"data_src.zip\"\n",
    "!unzip \"data_src.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Stage\n",
    "\n",
    "In the offline stage, we set up Pneuma, including initializing the database, registering dataset and metadata, generating summaries, and generating both vector and keyword index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# enforce more deterministic behavior in cuBLAS operations.\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "# select a GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from google.colab import userdata\n",
    "from pneuma import Pneuma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the pneuma object with out_path and call the setup() function to initialize the database. We have the option to use OpenAI LLM & embedding model, which default to `GPT-4o-mini` and `text-embedding-3-small`, respectively. Please set up `OPENAI_API_KEY` in your Colab instance. Alternatively, we can use local models by specifying the paths (`llm_path` and `embed_path`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The out_path is used to determine where the dataset and indexes will be stored.\n",
    "# If not set, it will be defaulted to the current working directory.\n",
    "out_path = \"out_demo\"\n",
    "USE_OPEN_AI = True\n",
    "\n",
    "if USE_OPEN_AI:\n",
    "    pneuma = Pneuma(\n",
    "        out_path=out_path,\n",
    "        openai_api_key=userdata.get('OPENAI_API_KEY'),\n",
    "        use_local_model=False,\n",
    "    )\n",
    "else:\n",
    "    pneuma = Pneuma(\n",
    "        out_path=out_path,\n",
    "        llm_path=\"Qwen/Qwen2.5-0.5B-Instruct\",  # We use a smaller model to fit in Colab\n",
    "        embed_path=\"BAAI/bge-base-en-v1.5\",\n",
    "        max_llm_batch_size=1,  # Limit exploration for limited Colab memory\n",
    "    )\n",
    "pneuma.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: For local LLMs, we limit exploration of dynamic batch size selector because it will fill the GPU memory quickly and not cleaned fast enough. This is not good for systems with limited GPU memory such as Colab with the T4 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we use a dataset of three tables taken from Chicago Open Data with the following descriptions:\n",
    "\n",
    "- **5cq6-qygt.csv**: Bus stops in shelters and at Chicago Transport Authority (CTA) rail stations which have digital signs added to them to show upcoming arrivals.\n",
    "- **5n77-2d6a.csv**: Survey results of the 12th ward residents about issues ranging from climate & sustainability to public safety.\n",
    "- **28km-gtjn.csv**: Fire stations location in Chicago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To register a dataset, we call the add_tables function while pointing to a directory and specifying the data creator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data_src/sample_data/csv\"\n",
    "response = pneuma.add_tables(path=data_path, creator=\"demo_user\")\n",
    "json.loads(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can summarize the tables, all of which are not yet summarized at this point. These summaries then represent the tables for the discovery process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pneuma.summarize()\n",
    "json.loads(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, if context (metadata) is available, we can register it as well using the add_metadata function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"data_src/sample_data/metadata.csv\"\n",
    "response = pneuma.add_metadata(metadata_path=metadata_path)\n",
    "json.loads(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Generation\n",
    "The summaries (and optionally metadata) need to be indexed into a hybrid retriever (combining vector and full-text indices). To do so, we call the generate_index function while specifying a name for the index. By default, this function will index all registered tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pneuma.generate_index(index_name=\"demo_index\")\n",
    "json.loads(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Stage (Querying)\n",
    "To retrieve a ranked list of tables, we use the query_index function. In this example, Pneuma correctly identifies all the relevant tables for the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Which dataset contains climate issues?\",  # 5n77-2d6a.csv\n",
    "    \"If I could identify where the bus stops are in Chicago, that would be awesome!\"  # 5cq6-qygt.csv\n",
    "]\n",
    "\n",
    "response = pneuma.query_index(\n",
    "    index_name=\"demo_index\",\n",
    "    queries=queries,\n",
    "    k=1,\n",
    "    n=5,\n",
    "    alpha=0.5,\n",
    ")\n",
    "relevant_tables = json.dumps(json.loads(response), indent=4)\n",
    "print(relevant_tables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pneuma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
